{
 "cells": [
  {
   "cell_type": "raw",
   "id": "247edc43",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Word Enbedding contains --> \n",
    "1. Frequency Based Embedding \n",
    "2. Prediction Based Embedding(Neural Network Based Embedding)\n",
    "\n",
    "1. Frequency Based Embedding:\n",
    "   a) Count Vector(Bow)\n",
    "   b) TF-IDF Vector\n",
    "\n",
    "2. Prediction Based Embedding(Neural Network Based Embedding):\n",
    "    a) Word2Vec\n",
    "        - CBOW(Continuous Bag of Words)\n",
    "        - Skip-Gram\n",
    "    b) GloVe(Global Vectors for Word Representation)\n",
    "    c) FastText\n",
    "        - Subword Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af5c48d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. NN Based Embedding Models\n",
    "# a) Word2Vec\n",
    "# Implementation of Word2Vec without using any external libraries.\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f9e7d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block 2 Input Sentence\n",
    "sentences = [\n",
    "     \"i love machine learning\",\n",
    "    \"machine learning is fun\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2fa28ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['is', 'fun', 'machine', 'love', 'i', 'learning']\n",
      "Word to Index: {'is': 0, 'fun': 1, 'machine': 2, 'love': 3, 'i': 4, 'learning': 5}\n",
      "Index to Word: {0: 'is', 1: 'fun', 2: 'machine', 3: 'love', 4: 'i', 5: 'learning'}\n"
     ]
    }
   ],
   "source": [
    "# bock 3 Tokenization + Vocabulary Creation\n",
    "words = []\n",
    "for sentence in sentences:\n",
    "    words.extend(sentence.split())\n",
    "\n",
    "vocab = list(set(words))  # Unique words  in the corpus (set removes duplicates)       \n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}   # Word to Index mapping \n",
    "index_to_word = {i: word for word, i in word_to_index.items()}  # Index to Word mapping\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Word to Index:\", word_to_index)\n",
    "print(\"Index to Word:\", index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8fadab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-Target Pairs:\n",
      "Context: ['i', 'machine'] -> Target: love\n",
      "Context: ['love', 'learning'] -> Target: machine\n",
      "Context: ['machine', 'is'] -> Target: learning\n",
      "Context: ['learning', 'fun'] -> Target: is\n"
     ]
    }
   ],
   "source": [
    "# block 4 Create CBOW Training pairs\n",
    "window_size = 1\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.split()\n",
    "    for i in range(window_size, len(tokens) - window_size):\n",
    "        context = [\n",
    "            tokens[i - 1],      # used to store previous word\n",
    "            tokens[i + 1]       # used to store next word\n",
    "        ]\n",
    "        target = tokens[i]  # used to store current word\n",
    "\n",
    "        X.append([word_to_index[w] for w in context])   # context words as indices\n",
    "        Y.append(word_to_index[target]) # target word as index\n",
    "        \n",
    "print(\"Context-Target Pairs:\")\n",
    "for i in range(len(X)):\n",
    "    context_words = [index_to_word[idx] for idx in X[i]]\n",
    "    target_word = index_to_word[Y[i]]\n",
    "    print(f\"Context: {context_words} -> Target: {target_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0d7b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#block 5 One-Hot Encoding\n",
    "def one_hot(index, size):\n",
    "    vec = np.zeros(size)\n",
    "    vec[index] = 1\n",
    "    return vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa760116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block 6 Initialize Weights\n",
    "embedding_dim = 5\n",
    "\n",
    "W1 = np.random.randn(vocab_size, embedding_dim)\n",
    "W2 = np.random.randn(embedding_dim, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebb371a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block 7 Softmax Function\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5374749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 11.260526887891208\n",
      "Epoch 100, Loss: 11.260526887891208\n",
      "Epoch 200, Loss: 11.260526887891208\n",
      "Epoch 300, Loss: 11.260526887891208\n",
      "Epoch 400, Loss: 11.260526887891208\n"
     ]
    }
   ],
   "source": [
    "# block 8 Forward Pass\n",
    "learning_rate = 0.01\n",
    "\n",
    "for epoch in range(500):\n",
    "    loss = 0\n",
    "\n",
    "    for context_indices, target_index in zip(X, Y):\n",
    "\n",
    "        # Context embeddings\n",
    "        h = np.mean([W1[idx] for idx in context_indices], axis=0)\n",
    "\n",
    "        # Prediction\n",
    "        u = np.dot(h, W2)\n",
    "        y_pred = softmax(u)\n",
    "\n",
    "        # Loss\n",
    "        loss -= np.log(y_pred[target_index])\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20707e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = W1[word_to_index[\"machine\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51cf3348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine\n"
     ]
    }
   ],
   "source": [
    "predicted_index = np.argmax(y_pred)  # Get index of the predicted word (1)\n",
    "predicted_word = index_to_word[predicted_index] # Convert index back to word\n",
    "print(predicted_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "605687dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Using Gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [\n",
    "    [\"i\", \"love\", \"machine\", \"learning\"],\n",
    "    [\"machine\", \"learning\", \"is\", \"fun\"],\n",
    "    [\"deep\", \"learning\", \"uses\", \"neural\", \"networks\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d565c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Cbow model\n",
    "model = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=10,      # it means each word is represented by a 5-dimensional vector\n",
    "    window=2,       #2 words before and after the target word\n",
    "    min_count=1,        #Include all words, even if they appear once\n",
    "    sg=0  # sg=0 --> CBOW, sg=1 --> Skip Gram\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22106d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "words = [\"deep\", \"learning\"]\n",
    "\n",
    "vectors = [model.wv[word] for word in words]\n",
    "combined_vector = np.mean(vectors, axis=0)  # Combine vectors by averaging\n",
    "\n",
    "print(combined_vector.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23992de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('learning', 0.6315835118293762),\n",
       " ('deep', 0.5737736225128174),\n",
       " ('love', 0.544927179813385),\n",
       " ('neural', 0.16401201486587524),\n",
       " ('machine', 0.15487389266490936)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_vector(combined_vector, topn=5)  # Find top 5 similar words based on the combined vector\n",
    "\n",
    "# cbow find semantic simalrity after the combined_vector by wv.similarity, uses cosine similarity\n",
    "# but during training it uses dot product similarity predicts target words\n",
    "# same is followed by the skip gram "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17aa736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314fc52f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
